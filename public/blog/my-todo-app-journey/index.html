<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>My Todo App Journey An Automated Cloud-Native Platform | Kyle Mapue</title>
<meta name="keywords" content="">
<meta name="description" content="Introduction: The Project That Taught Me Everything
In the world of DevOps, you can read documentation for months, but nothing solidifies your skills like building something real. For me, that &ldquo;something&rdquo; was a seemingly simple Todo application. This project, inspired by the practical challenges presented in devopswithkubernetes.com, became my personal deep dive into Kubernetes, taking me from the humble beginnings of kubectl apply on my local machine to orchestrating a state-of-the-art, multi-environment GitOps workflow on Google Kubernetes Engine (GKE).">
<meta name="author" content="Kyle Mapue">
<link rel="canonical" href="http://localhost:1313/blog/my-todo-app-journey/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8943a90491e468f6ef8d33a5ef0a5845622d5178e663849217af60f670605c5b.css" integrity="sha256-iUOpBJHkaPbvjTOl7wpYRWItUXjmY4SSF69g9nBgXFs=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/images/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blog/my-todo-app-journey/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/blog/my-todo-app-journey/">
  <meta property="og:site_name" content="Kyle Mapue">
  <meta property="og:title" content="My Todo App Journey An Automated Cloud-Native Platform">
  <meta property="og:description" content="Introduction: The Project That Taught Me Everything In the world of DevOps, you can read documentation for months, but nothing solidifies your skills like building something real. For me, that “something” was a seemingly simple Todo application. This project, inspired by the practical challenges presented in devopswithkubernetes.com, became my personal deep dive into Kubernetes, taking me from the humble beginnings of kubectl apply on my local machine to orchestrating a state-of-the-art, multi-environment GitOps workflow on Google Kubernetes Engine (GKE).">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2025-07-28T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-07-28T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="My Todo App Journey An Automated Cloud-Native Platform">
<meta name="twitter:description" content="Introduction: The Project That Taught Me Everything
In the world of DevOps, you can read documentation for months, but nothing solidifies your skills like building something real. For me, that &ldquo;something&rdquo; was a seemingly simple Todo application. This project, inspired by the practical challenges presented in devopswithkubernetes.com, became my personal deep dive into Kubernetes, taking me from the humble beginnings of kubectl apply on my local machine to orchestrating a state-of-the-art, multi-environment GitOps workflow on Google Kubernetes Engine (GKE).">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "My Todo App Journey An Automated Cloud-Native Platform",
      "item": "http://localhost:1313/blog/my-todo-app-journey/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "My Todo App Journey An Automated Cloud-Native Platform",
  "name": "My Todo App Journey An Automated Cloud-Native Platform",
  "description": "Introduction: The Project That Taught Me Everything In the world of DevOps, you can read documentation for months, but nothing solidifies your skills like building something real. For me, that \u0026ldquo;something\u0026rdquo; was a seemingly simple Todo application. This project, inspired by the practical challenges presented in devopswithkubernetes.com, became my personal deep dive into Kubernetes, taking me from the humble beginnings of kubectl apply on my local machine to orchestrating a state-of-the-art, multi-environment GitOps workflow on Google Kubernetes Engine (GKE).\n",
  "keywords": [
    
  ],
  "articleBody": "Introduction: The Project That Taught Me Everything In the world of DevOps, you can read documentation for months, but nothing solidifies your skills like building something real. For me, that “something” was a seemingly simple Todo application. This project, inspired by the practical challenges presented in devopswithkubernetes.com, became my personal deep dive into Kubernetes, taking me from the humble beginnings of kubectl apply on my local machine to orchestrating a state-of-the-art, multi-environment GitOps workflow on Google Kubernetes Engine (GKE).\nThis post is the story of that journey. It’s a chronicle of the real-world problems, the late-night debugging sessions, and the architectural “aha!” moments that are often left out of the tutorials. It’s proof that by embracing complexity and solving problems from the ground up, you can build not just an application, but a truly professional and resilient platform. This is how I went from knowing Kubernetes to understanding it.\nChapter 1: The Application Stack - A Tale of Three Microservices The project started with a simple goal: build a Todo app. To truly learn microservice architecture, I broke it down into three distinct services, each with its own responsibility:\nThe todo-backend: The heart of the operation. A Node.js API that handles all business logic, from creating tasks to marking them as done. It is the stateful component, connected to a PostgreSQL database. The todo-frontend: The face of the application. A Node.js server that renders the user interface, making calls to the backend to manage tasks. The broadcaster: The town crier. A decoupled notification service that listens for events (like “new todo created”) from the backend via a NATS message queue and forwards them to a Discord channel. This separation was my first major architectural decision, forcing me to think about how services discover and communicate with each other inside a Kubernetes cluster.\nChapter 2: The First Real-World Failure: The Unready Pod My first deployment to GKE was met with a classic, humbling problem: the todo-backend pod was stuck in a 0/1 READY state. The logs revealed the truth: FATAL: password authentication failed for user \"todo_user\".\nThe “aha!” moment was realizing that the PostgreSQL StatefulSet wasn’t running its initialization script because its PersistentVolumeClaim (PVC) already contained data from a previous failed attempt. The database thought it was already set up. This taught me a critical lesson in managing stateful applications: you have to understand the lifecycle of your data. The only solution was to completely delete the PVC and the StatefulSet to force a truly clean start.\nChapter 3: Building a Declarative Foundation with Kustomize With the application running, it was time to professionalize the deployment process. I abandoned scattered kubectl apply commands and embraced a declarative approach using Kustomize. This was the cornerstone of the entire GitOps structure.\nI created a new, dedicated todo-config repository to act as the single source of truth for my cluster’s state. The structure is clean and powerful:\nbase/: Contains the generic, shared manifests for all services. overlays/: Contains environment-specific patches. This is where the magic happens. In overlays/staging, I patch the broadcaster to disable Discord notifications and remove the database backup CronJob. In overlays/production, I use a different patch to enable these features. This structure means my base configuration is clean and reusable, and the differences between my environments are explicitly and clearly defined in code.\nChapter 4: The CI/CD Pipeline - Automating Staging and Production With a declarative configuration in place, I built the automation engine with GitHub Actions. Each of the three microservice repositories received its own independent CI workflow. This pipeline perfectly follows the instructions I set for myself, enabling multiple branch-specific environments:\nDeploy to Staging: A git push to the main branch of any service kicks off the workflow. It builds a new image and updates the kustomization.yaml in the staging overlay of the todo-config repo. Deploy to Production: When I’m ready for a production release, I create a version tag (e.g., v1.2.0). The same workflow detects this and, instead of updating the staging overlay, it updates the production overlay. This is the GitOps dream in action. My cluster state is now a direct reflection of my Git history, and deployments are a matter of a simple git push or git tag.\nChapter 5: State-of-the-Art Monitoring with Prometheus and Grafana A running application is one thing; a healthy application is another. To achieve true production readiness, I built a robust monitoring stack using the industry-standard tools: Prometheus and Grafana.\nI deployed a production-ready NATS message queue using its official Helm chart, and with a simple configuration change, I enabled its Prometheus metrics endpoint.\n# values.yaml for the NATS Helm chart metrics: enabled: true serviceMonitor: enabled: true namespace: prometheus # The namespace where Prometheus is running This serviceMonitor object is the key. It tells Prometheus how to automatically discover and start scraping metrics from my NATS cluster. After port-forwarding to the Prometheus pod, I could instantly verify that data was flowing with a simple query\nWith metrics flowing, the final step was visualization. I port-forwarded to the Grafana service, imported a pre-built NATS dashboard, and was immediately greeted with a real-time overview of my entire messaging infrastructure: CPU and memory usage, message throughput, and active client connections. This isn’t just a pretty graph; it’s the command center for my application’s health.\nWith metrics flowing, the final step was visualization. I port-forwarded to the Grafana service, imported a pre-built NATS dashboard, and was immediately greeted with a real-time overview of my entire messaging infrastructure: CPU and memory usage, message throughput, and active client connections. This isn’t just a pretty graph; it’s the command center for my application’s health.\nChapter 6: Advanced Deployments - Canary Releases with Automatic Rollback Deploying directly to production is risky. To mitigate this, I implemented an advanced Canary release strategy using Argo Rollouts. This is where the power of the monitoring stack truly shines.\nInstead of a standard Kubernetes Deployment, I now use an Argo Rollout resource. This allows me to define a precise, automated release process:\nInitial Canary: When a new version is deployed to production, Argo Rollouts sends only 20% of live traffic to it. Automated Analysis: For the next minute, it pauses. During this time, an AnalysisTemplate I wrote continuously runs a Prometheus query to check the application’s CPU usage. The Safety Net: If the CPU usage for the new version remains below my defined threshold (result[0] \u003c= 0.5), the rollout proceeds, gradually shifting more traffic to the new version. Automatic Malfunction Detection: If the CPU usage spikes, the AnalysisTemplate fails. Argo Rollouts immediately and automatically aborts the deployment and rolls back to the previous stable version. No human intervention is required. # A snippet from my AnalysisTemplate ... metrics: - name: cpu-usage-sum interval: 1m count: 5 provider: prometheus: address: http://prometheus-kube-prometheus-prometheus.prometheus.svc.cluster.local:9090 query: | sum(rate(container_cpu_usage_seconds_total{namespace='{{args.namespace}}', container!=''}[5m])) successCondition: \"result[0] \u003c= 0.5\" failureCondition: \"result[0] \u003e 0.5\" This is the pinnacle of a modern CI/CD pipeline: not just automated deployment, but automated, metric-driven safety.\nChapter 7: Professional Touches - Routing and Secrets With the core application and deployment strategy in place, I tackled two final topics to elevate the project to a professional standard.\n1. Hostname-Based Routing with the Gateway API: Instead of messy path-based routing (/staging/todo), I implemented the industry-standard hostname-based routing. This keeps my application code clean and environment-agnostic.\nHow it Works: I used Kustomize overlays to patch my HTTPRoute manifest. The staging overlay sets the hostname to staging.34.36.101.217.nip.io, while the production overlay sets it to todo.34.36.101.217.nip.io. The nip.io service provides a free and seamless way to get wildcard DNS for any IP, allowing me to use professional routing patterns without configuring a real domain. 2. Secrets Management: The instructions allowed me to assume secrets were handled externally, but a professional engineer plans for reality. In a real system, you never commit plaintext secrets to Git.\nThe “Easy” Way (What I did for now): I manually created the secrets in the cluster using kubectl create secret. This works, but it’s not repeatable. The “Right” Way (The Next Step): The industry-standard solution is to use a tool like HashiCorp Vault or GCP Secret Manager combined with the External Secrets Operator. The operator would run in my cluster, read secret references from my Git repository, and securely fetch the actual secret values from the vault, creating the Kubernetes secrets just in time for my application to use them. This is the most secure and scalable approach. Chapter 8: Real-World Refinements - Confronting State As the platform stabilized, I took a step back to critically review my own work. A good engineer knows that the job isn’t just about making things work, but making them work correctly and efficiently. This review uncovered several important discrepancies—the kind of subtle but significant issues that separate a prototype from a production-grade system.\n1. The Stateful Frontend: A Lesson in ReadWriteOnce\nMy biggest “aha!” moment during the review was realizing I had made my frontend stateful, and understanding the implications.\nThe Goal: I wanted the random image on the frontend to be persistent. If a pod died, I didn’t want to wait for a new image to be downloaded. I correctly identified that this required a PersistentVolume. The Implementation: I attached a PersistentVolumeClaim to my todo-frontend Deployment. This worked perfectly for a single replica, and it was a fantastic hands-on lesson in how pods can claim and use persistent storage. The Hidden Flaw: However, this approach uses a ReadWriteOnce (RWO) volume. I learned this means the underlying storage can only be mounted by one pod at a time. It was a scaling time bomb. If I had scaled the frontend to two replicas, the second pod would have failed to start because the volume was already “locked” by the first. The “Right” Way for Shared State: To achieve my goal of sharing one volume across multiple replicas, I would need a ReadWriteMany (RWX) volume, likely backed by a network file system like Google Cloud Filestore. This is a more advanced solution, but it’s the correct pattern for shared, writable state. For now, I’m keeping the RWO volume as a concrete reminder of this crucial lesson, but I know the path forward for a truly scalable solution. 2. The Image Tag Mismatch Dance: When GitOps Gets Stuck\nEven with the correct Project ID, my pods were still failing to pull images, reporting NotFound for tags like staging-initial. This was a crucial lesson in the timing of GitOps.\nThe Problem: My base Kustomize configuration specified newTag: staging-initial for the images. Kubernetes tried to pull these images immediately. However, my CI/CD pipeline builds images with SHA-based tags (e.g., staging-b287111) and then updates the kustomization.yaml in the overlays/staging directory with these new, correct tags. The staging-initial image simply didn’t exist in my GCR. The Complexity: This creates a race condition. The initial deployment attempts to pull a non-existent image, causing pods to get stuck in ImagePullBackOff. Even when the CI/CD pipeline successfully updates the Git repository with the correct SHA-based tags, the existing, failing pods might not immediately pick up the new image reference or restart. They remain in a persistent retry loop for the old, non-existent image. The Solution (Next Step): To unblock the current deployment, I need to ensure that an image with the staging-initial tag exists in my GCR for each service. Alternatively, I can force a rollout of the deployments to make them pick up the latest image references from the updated Kustomize configuration. 3. The Dependency Drift: A Tale of Two package.jsons\nI discovered a classic Node.js problem in the broadcaster service.\nThe Mistake: The package.json and package-lock.json files were out of sync. The lock file contained a different version of axios and even included winston, a package that wasn’t listed as a dependency at all. This is a recipe for “it works on my machine” bugs. The Fix: The fix was simple but crucial: I ran npm install in the service’s directory and committed the updated package.json. This ensures that both local development and the CI/CD pipeline build from the exact same dependency tree. 4. The Inefficiency: A Heavy Frontend for a Light Job\nMy frontend was over-engineered.\nThe Mistake: I was using a full Node.js Koa server to serve what was essentially a single, dynamically generated HTML file. My own Dockerfile even had a commented-out nginx implementation that I had ignored. The Fix (Next Step): The plan is to refactor this into a multi-stage Docker build. A node stage will build the static HTML file, and a final, lightweight nginx stage will serve it. This will result in a smaller, faster, and more secure container. Chapter 9: The Kustomize Conundrum and Service Connectivity Just when I thought the deployment was stable, new challenges emerged, highlighting the intricate dance between Kustomize, ArgoCD, and inter-service communication.\n1. The Stubborn ImagePullBackOff: Kustomize Overlays vs. Reality\nEven after correctly configuring GCR project IDs and ensuring my CI/CD pushed SHA-tagged images, pods were still stuck in ImagePullBackOff, trying to pull images with the staging-initial tag.\nThe Problem: My base Kustomize configuration specified newTag: staging-initial for the images. Kubernetes tried to pull these images immediately. However, my CI/CD pipeline builds images with SHA-based tags (e.g., staging-b287111) and then updates the kustomization.yaml in the overlays/staging directory with these new, correct tags. The staging-initial image simply didn’t exist in my GCR. The Docker Hub Detour (and why it was a band-aid): In an attempt to unblock, I temporarily switched the overlays/staging/kustomization.yaml to point to Docker Hub images. This was a quick fix to get any image pulled, but it wasn’t the long-term solution as my CI/CD pipeline pushes to GCR. The Strategic Merge Patch Breakthrough: The real solution came from understanding Kustomize’s patching mechanisms more deeply. Instead of relying on the images: field (which seemed to be problematic for direct overrides), I created explicit strategic merge patch files for each deployment (broadcaster, todo-backend, todo-frontend). These patches directly set the image field of the container to the full GCR image path including the SHA tag (e.g., gcr.io/dwk-gke-458706/broadcaster:staging-b287111). This forced the Kubernetes Deployment objects to update their image references. The Reversion: Once this explicit patching strategy proved successful in getting the pods to pull the correct images, I reverted these hardcoded strategic merge patches. The goal is for the CI/CD pipeline to dynamically update the images: field in overlays/staging/kustomization.yaml, and for ArgoCD to correctly apply that. This is the next phase of refinement. 2. The Broadcaster’s Identity Crisis: NATS Connectivity\nEven after the image issues were resolved, the broadcaster pod was still crashing with getaddrinfo ENOTFOUND my-nats.\nThe Problem: The broadcaster service was trying to connect to the NATS server using the service name my-nats. While my-nats exists, it was deployed in the default namespace, and the broadcaster was in the staging namespace. Kubernetes service discovery requires the fully qualified domain name (FQDN) for cross-namespace communication. The Solution: I updated the NATS_URL environment variable in the broadcaster-deployment.yaml to use the FQDN: nats://my-nats.default.svc.cluster.local:4222. This allowed the broadcaster to correctly resolve and connect to the NATS service. Staging Mode Confirmation: This also confirmed that my existing broadcaster-patch.yaml (which sets DISCORD_WEBHOOK_URL to an empty string) was correctly being applied, ensuring the broadcaster runs in a safe, log-only mode in staging. Overall Application Look Here are some screenshots of the overall application:\nConclusion: I Didn’t Just Build an App, I Built a Platform This Todo app journey was my crucible. It took me from the basics of Pods and Services to the advanced, real-world challenges of stateful sets, multi-environment deployments, and secure GitOps workflows. I didn’t just learn a list of commands; I learned how to think like a cloud-native engineer.\nI now have the skills to design, build, and manage a resilient, observable, and fully automated system on Kubernetes. This project is more than just a portfolio piece; it’s a testament to the deep, practical knowledge that can only be gained by building. And I’m ready to build what’s next.\n",
  "wordCount" : "2646",
  "inLanguage": "en",
  "datePublished": "2025-07-28T00:00:00Z",
  "dateModified": "2025-07-28T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Kyle Mapue"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blog/my-todo-app-journey/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Kyle Mapue",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/images/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Kyle Mapue (Alt + H)">
                <img src="http://localhost:1313/images/coding.png" alt="" aria-label="logo"
                    height="35">Kyle Mapue</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blog" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blog/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      My Todo App Journey An Automated Cloud-Native Platform
    </h1>
    <div class="post-meta"><span title='2025-07-28 00:00:00 +0000 UTC'>July 28, 2025</span>&nbsp;·&nbsp;Kyle Mapue

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">‎ Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#introduction-the-project-that-taught-me-everything" aria-label="Introduction: The Project That Taught Me Everything">Introduction: The Project That Taught Me Everything</a></li>
                <li>
                    <a href="#chapter-1-the-application-stack---a-tale-of-three-microservices" aria-label="Chapter 1: The Application Stack - A Tale of Three Microservices">Chapter 1: The Application Stack - A Tale of Three Microservices</a></li>
                <li>
                    <a href="#chapter-2-the-first-real-world-failure-the-unready-pod" aria-label="Chapter 2: The First Real-World Failure: The Unready Pod">Chapter 2: The First Real-World Failure: The Unready Pod</a></li>
                <li>
                    <a href="#chapter-3-building-a-declarative-foundation-with-kustomize" aria-label="Chapter 3: Building a Declarative Foundation with Kustomize">Chapter 3: Building a Declarative Foundation with Kustomize</a></li>
                <li>
                    <a href="#chapter-4-the-cicd-pipeline---automating-staging-and-production" aria-label="Chapter 4: The CI/CD Pipeline - Automating Staging and Production">Chapter 4: The CI/CD Pipeline - Automating Staging and Production</a></li>
                <li>
                    <a href="#chapter-5-state-of-the-art-monitoring-with-prometheus-and-grafana" aria-label="Chapter 5: State-of-the-Art Monitoring with Prometheus and Grafana">Chapter 5: State-of-the-Art Monitoring with Prometheus and Grafana</a></li>
                <li>
                    <a href="#chapter-6-advanced-deployments---canary-releases-with-automatic-rollback" aria-label="Chapter 6: Advanced Deployments - Canary Releases with Automatic Rollback">Chapter 6: Advanced Deployments - Canary Releases with Automatic Rollback</a></li>
                <li>
                    <a href="#chapter-7-professional-touches---routing-and-secrets" aria-label="Chapter 7: Professional Touches - Routing and Secrets">Chapter 7: Professional Touches - Routing and Secrets</a></li>
                <li>
                    <a href="#chapter-8-real-world-refinements---confronting-state" aria-label="Chapter 8: Real-World Refinements - Confronting State">Chapter 8: Real-World Refinements - Confronting State</a></li>
                <li>
                    <a href="#chapter-9-the-kustomize-conundrum-and-service-connectivity" aria-label="Chapter 9: The Kustomize Conundrum and Service Connectivity">Chapter 9: The Kustomize Conundrum and Service Connectivity</a></li></ul>
                    
                <li>
                    <a href="#overall-application-look" aria-label="Overall Application Look">Overall Application Look</a><ul>
                        
                <li>
                    <a href="#conclusion-i-didn" aria-label="Conclusion: I Didn&rsquo;t Just Build an App, I Built a Platform">Conclusion: I Didn&rsquo;t Just Build an App, I Built a Platform</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h4 id="introduction-the-project-that-taught-me-everything"><strong>Introduction: The Project That Taught Me Everything</strong><a hidden class="anchor" aria-hidden="true" href="#introduction-the-project-that-taught-me-everything">#</a></h4>
<p>In the world of DevOps, you can read documentation for months, but nothing solidifies your skills like building something real. For me, that &ldquo;something&rdquo; was a seemingly simple Todo application. This project, inspired by the practical challenges presented in <code>devopswithkubernetes.com</code>, became my personal deep dive into Kubernetes, taking me from the humble beginnings of <code>kubectl apply</code> on my local machine to orchestrating a state-of-the-art, multi-environment GitOps workflow on Google Kubernetes Engine (GKE).</p>
<p>This post is the story of that journey. It’s a chronicle of the real-world problems, the late-night debugging sessions, and the architectural &ldquo;aha!&rdquo; moments that are often left out of the tutorials. It’s proof that by embracing complexity and solving problems from the ground up, you can build not just an application, but a truly professional and resilient platform. This is how I went from knowing Kubernetes to <em>understanding</em> it.</p>
<h4 id="chapter-1-the-application-stack---a-tale-of-three-microservices"><strong>Chapter 1: The Application Stack - A Tale of Three Microservices</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-1-the-application-stack---a-tale-of-three-microservices">#</a></h4>
<p>The project started with a simple goal: build a Todo app. To truly learn microservice architecture, I broke it down into three distinct services, each with its own responsibility:</p>
<ul>
<li><strong>The <code>todo-backend</code>:</strong> The heart of the operation. A Node.js API that handles all business logic, from creating tasks to marking them as done. It is the stateful component, connected to a PostgreSQL database.</li>
<li><strong>The <code>todo-frontend</code>:</strong> The face of the application. A Node.js server that renders the user interface, making calls to the backend to manage tasks.</li>
<li><strong>The <code>broadcaster</code>:</strong> The town crier. A decoupled notification service that listens for events (like &ldquo;new todo created&rdquo;) from the backend via a NATS message queue and forwards them to a Discord channel.</li>
</ul>
<p>This separation was my first major architectural decision, forcing me to think about how services discover and communicate with each other inside a Kubernetes cluster.</p>
<h4 id="chapter-2-the-first-real-world-failure-the-unready-pod"><strong>Chapter 2: The First Real-World Failure: The Unready Pod</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-2-the-first-real-world-failure-the-unready-pod">#</a></h4>
<p>My first deployment to GKE was met with a classic, humbling problem: the <code>todo-backend</code> pod was stuck in a <code>0/1 READY</code> state. The logs revealed the truth: <code>FATAL: password authentication failed for user &quot;todo_user&quot;</code>.</p>
<p>The &ldquo;aha!&rdquo; moment was realizing that the PostgreSQL <code>StatefulSet</code> wasn&rsquo;t running its initialization script because its <code>PersistentVolumeClaim</code> (PVC) already contained data from a previous failed attempt. The database thought it was already set up. This taught me a critical lesson in managing stateful applications: <strong>you have to understand the lifecycle of your data.</strong> The only solution was to completely delete the PVC and the <code>StatefulSet</code> to force a truly clean start.</p>
<h4 id="chapter-3-building-a-declarative-foundation-with-kustomize"><strong>Chapter 3: Building a Declarative Foundation with Kustomize</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-3-building-a-declarative-foundation-with-kustomize">#</a></h4>
<p>With the application running, it was time to professionalize the deployment process. I abandoned scattered <code>kubectl apply</code> commands and embraced a declarative approach using <strong>Kustomize</strong>. This was the cornerstone of the entire GitOps structure.</p>
<p>I created a new, dedicated <code>todo-config</code> repository to act as the single source of truth for my cluster&rsquo;s state. The structure is clean and powerful:</p>
<ul>
<li><strong><code>base/</code></strong>: Contains the generic, shared manifests for all services.</li>
<li><strong><code>overlays/</code></strong>: Contains environment-specific patches. This is where the magic happens.
<ul>
<li>In <code>overlays/staging</code>, I patch the <code>broadcaster</code> to disable Discord notifications and remove the database backup <code>CronJob</code>.</li>
<li>In <code>overlays/production</code>, I use a different patch to enable these features.</li>
</ul>
</li>
</ul>
<p>This structure means my base configuration is clean and reusable, and the differences between my environments are explicitly and clearly defined in code.</p>
<h4 id="chapter-4-the-cicd-pipeline---automating-staging-and-production"><strong>Chapter 4: The CI/CD Pipeline - Automating Staging and Production</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-4-the-cicd-pipeline---automating-staging-and-production">#</a></h4>
<p>With a declarative configuration in place, I built the automation engine with <strong>GitHub Actions</strong>. Each of the three microservice repositories received its own independent CI workflow. This pipeline perfectly follows the instructions I set for myself, enabling <strong>multiple branch-specific environments</strong>:</p>
<ol>
<li><strong>Deploy to Staging:</strong> A <code>git push</code> to the <code>main</code> branch of any service kicks off the workflow. It builds a new image and updates the <code>kustomization.yaml</code> in the <strong>staging</strong> overlay of the <code>todo-config</code> repo.</li>
<li><strong>Deploy to Production:</strong> When I&rsquo;m ready for a production release, I create a version tag (e.g., <code>v1.2.0</code>). The same workflow detects this and, instead of updating the staging overlay, it updates the <strong>production</strong> overlay.</li>
</ol>
<p>This is the GitOps dream in action. My cluster state is now a direct reflection of my Git history, and deployments are a matter of a simple <code>git push</code> or <code>git tag</code>.</p>
<h4 id="chapter-5-state-of-the-art-monitoring-with-prometheus-and-grafana"><strong>Chapter 5: State-of-the-Art Monitoring with Prometheus and Grafana</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-5-state-of-the-art-monitoring-with-prometheus-and-grafana">#</a></h4>
<p>A running application is one thing; a <em>healthy</em> application is another. To achieve true production readiness, I built a robust monitoring stack using the industry-standard tools: <strong>Prometheus</strong> and <strong>Grafana</strong>.</p>
<p>I deployed a production-ready NATS message queue using its official Helm chart, and with a simple configuration change, I enabled its Prometheus metrics endpoint.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># values.yaml for the NATS Helm chart</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metrics</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">serviceMonitor</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">prometheus</span> <span style="color:#75715e"># The namespace where Prometheus is running</span>
</span></span></code></pre></div><p>This <code>serviceMonitor</code> object is the key. It tells Prometheus how to automatically discover and start scraping metrics from my NATS cluster. After port-forwarding to the Prometheus pod, I could instantly verify that data was flowing with a simple query</p>
<p>With metrics flowing, the final step was visualization. I port-forwarded to the Grafana service, imported a pre-built NATS dashboard, and was immediately greeted with a real-time overview of my entire messaging infrastructure: CPU and memory usage, message throughput, and active client connections. This isn&rsquo;t just a pretty graph; it&rsquo;s the command center for my application&rsquo;s health.</p>
<p>With metrics flowing, the final step was visualization. I port-forwarded to the Grafana service, imported a pre-built NATS dashboard, and was immediately greeted with a real-time overview of my entire messaging infrastructure: CPU and memory usage, message throughput, and active client connections. This isn&rsquo;t just a pretty graph; it&rsquo;s the command center for my application&rsquo;s health.</p>
<h4 id="chapter-6-advanced-deployments---canary-releases-with-automatic-rollback"><strong>Chapter 6: Advanced Deployments - Canary Releases with Automatic Rollback</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-6-advanced-deployments---canary-releases-with-automatic-rollback">#</a></h4>
<p>Deploying directly to production is risky. To mitigate this, I implemented an advanced <strong>Canary release strategy</strong> using <strong>Argo Rollouts</strong>. This is where the power of the monitoring stack truly shines.</p>
<p>Instead of a standard Kubernetes <code>Deployment</code>, I now use an Argo <code>Rollout</code> resource. This allows me to define a precise, automated release process:</p>
<ol>
<li><strong>Initial Canary:</strong> When a new version is deployed to production, Argo Rollouts sends only 20% of live traffic to it.</li>
<li><strong>Automated Analysis:</strong> For the next minute, it pauses. During this time, an <code>AnalysisTemplate</code> I wrote continuously runs a Prometheus query to check the application&rsquo;s CPU usage.</li>
<li><strong>The Safety Net:</strong> If the CPU usage for the new version remains below my defined threshold (<code>result[0] &lt;= 0.5</code>), the rollout proceeds, gradually shifting more traffic to the new version.</li>
<li><strong>Automatic Malfunction Detection:</strong> If the CPU usage spikes, the <code>AnalysisTemplate</code> fails. Argo Rollouts immediately and <strong>automatically aborts the deployment and rolls back</strong> to the previous stable version. No human intervention is required.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># A snippet from my AnalysisTemplate</span>
</span></span><span style="display:flex;"><span>...  
</span></span><span style="display:flex;"><span><span style="color:#f92672">metrics</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cpu-usage-sum</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">interval</span>: <span style="color:#ae81ff">1m</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">count</span>: <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">provider</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">prometheus</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">address</span>: <span style="color:#ae81ff">http://prometheus-kube-prometheus-prometheus.prometheus.svc.cluster.local:9090</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">query</span>: |<span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        sum(rate(container_cpu_usage_seconds_total{namespace=&#39;{{args.namespace}}&#39;, container!=&#39;&#39;}[5m]))</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">successCondition</span>: <span style="color:#e6db74">&#34;result[0] &lt;= 0.5&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">failureCondition</span>: <span style="color:#e6db74">&#34;result[0] &gt; 0.5&#34;</span>
</span></span></code></pre></div><p>This is the pinnacle of a modern CI/CD pipeline: not just automated deployment, but automated, metric-driven safety.</p>
<h4 id="chapter-7-professional-touches---routing-and-secrets"><strong>Chapter 7: Professional Touches - Routing and Secrets</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-7-professional-touches---routing-and-secrets">#</a></h4>
<p>With the core application and deployment strategy in place, I tackled two final topics to elevate the project to a professional standard.</p>
<p><strong>1. Hostname-Based Routing with the Gateway API:</strong>
Instead of messy path-based routing (<code>/staging/todo</code>), I implemented the industry-standard hostname-based routing. This keeps my application code clean and environment-agnostic.</p>
<ul>
<li><strong>How it Works:</strong> I used Kustomize overlays to patch my <code>HTTPRoute</code> manifest. The <code>staging</code> overlay sets the hostname to <code>staging.34.36.101.217.nip.io</code>, while the <code>production</code> overlay sets it to <code>todo.34.36.101.217.nip.io</code>. The <code>nip.io</code> service provides a free and seamless way to get wildcard DNS for any IP, allowing me to use professional routing patterns without configuring a real domain.</li>
</ul>
<p><strong>2. Secrets Management:</strong>
The instructions allowed me to assume secrets were handled externally, but a professional engineer plans for reality. In a real system, you <strong>never</strong> commit plaintext secrets to Git.</p>
<ul>
<li><strong>The &ldquo;Easy&rdquo; Way (What I did for now):</strong> I manually created the secrets in the cluster using <code>kubectl create secret</code>. This works, but it&rsquo;s not repeatable.</li>
<li><strong>The &ldquo;Right&rdquo; Way (The Next Step):</strong> The industry-standard solution is to use a tool like <strong>HashiCorp Vault</strong> or <strong>GCP Secret Manager</strong> combined with the <strong>External Secrets Operator</strong>. The operator would run in my cluster, read secret <em>references</em> from my Git repository, and securely fetch the actual secret values from the vault, creating the Kubernetes secrets just in time for my application to use them. This is the most secure and scalable approach.</li>
</ul>
<h4 id="chapter-8-real-world-refinements---confronting-state"><strong>Chapter 8: Real-World Refinements - Confronting State</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-8-real-world-refinements---confronting-state">#</a></h4>
<p>As the platform stabilized, I took a step back to critically review my own work. A good engineer knows that the job isn&rsquo;t just about making things work, but making them work <em>correctly</em> and <em>efficiently</em>. This review uncovered several important discrepancies—the kind of subtle but significant issues that separate a prototype from a production-grade system.</p>
<p><strong>1. The Stateful Frontend: A Lesson in <code>ReadWriteOnce</code></strong></p>
<p>My biggest &ldquo;aha!&rdquo; moment during the review was realizing I had made my frontend stateful, and understanding the implications.</p>
<ul>
<li><strong>The Goal:</strong> I wanted the random image on the frontend to be persistent. If a pod died, I didn&rsquo;t want to wait for a new image to be downloaded. I correctly identified that this required a <code>PersistentVolume</code>.</li>
<li><strong>The Implementation:</strong> I attached a <code>PersistentVolumeClaim</code> to my <code>todo-frontend</code> <code>Deployment</code>. This worked perfectly for a single replica, and it was a fantastic hands-on lesson in how pods can claim and use persistent storage.</li>
<li><strong>The Hidden Flaw:</strong> However, this approach uses a <code>ReadWriteOnce</code> (RWO) volume. I learned this means the underlying storage can only be mounted by <strong>one pod at a time</strong>. It was a scaling time bomb. If I had scaled the frontend to two replicas, the second pod would have failed to start because the volume was already &ldquo;locked&rdquo; by the first.</li>
<li><strong>The &ldquo;Right&rdquo; Way for Shared State:</strong> To achieve my goal of sharing one volume across multiple replicas, I would need a <code>ReadWriteMany</code> (RWX) volume, likely backed by a network file system like Google Cloud Filestore. This is a more advanced solution, but it&rsquo;s the correct pattern for shared, writable state. For now, I&rsquo;m keeping the RWO volume as a concrete reminder of this crucial lesson, but I know the path forward for a truly scalable solution.</li>
</ul>
<p><strong>2. The Image Tag Mismatch Dance: When GitOps Gets Stuck</strong></p>
<p>Even with the correct Project ID, my pods were still failing to pull images, reporting <code>NotFound</code> for tags like <code>staging-initial</code>. This was a crucial lesson in the timing of GitOps.</p>
<ul>
<li><strong>The Problem:</strong> My <code>base</code> Kustomize configuration specified <code>newTag: staging-initial</code> for the images. Kubernetes tried to pull these images immediately. However, my CI/CD pipeline builds images with SHA-based tags (e.g., <code>staging-b287111</code>) and then updates the <code>kustomization.yaml</code> in the <code>overlays/staging</code> directory with these new, correct tags. The <code>staging-initial</code> image simply didn&rsquo;t exist in my GCR.</li>
<li><strong>The Complexity:</strong> This creates a race condition. The initial deployment attempts to pull a non-existent image, causing pods to get stuck in <code>ImagePullBackOff</code>. Even when the CI/CD pipeline successfully updates the Git repository with the correct SHA-based tags, the existing, failing pods might not immediately pick up the new image reference or restart. They remain in a persistent retry loop for the old, non-existent image.</li>
<li><strong>The Solution (Next Step):</strong> To unblock the current deployment, I need to ensure that an image with the <code>staging-initial</code> tag exists in my GCR for each service. Alternatively, I can force a rollout of the deployments to make them pick up the latest image references from the updated Kustomize configuration.</li>
</ul>
<p><strong>3. The Dependency Drift: A Tale of Two <code>package.json</code>s</strong></p>
<p>I discovered a classic Node.js problem in the <code>broadcaster</code> service.</p>
<ul>
<li><strong>The Mistake:</strong> The <code>package.json</code> and <code>package-lock.json</code> files were out of sync. The lock file contained a different version of <code>axios</code> and even included <code>winston</code>, a package that wasn&rsquo;t listed as a dependency at all. This is a recipe for &ldquo;it works on my machine&rdquo; bugs.</li>
<li><strong>The Fix:</strong> The fix was simple but crucial: I ran <code>npm install</code> in the service&rsquo;s directory and committed the updated <code>package.json</code>. This ensures that both local development and the CI/CD pipeline build from the exact same dependency tree.</li>
</ul>
<p><strong>4. The Inefficiency: A Heavy Frontend for a Light Job</strong></p>
<p>My frontend was over-engineered.</p>
<ul>
<li><strong>The Mistake:</strong> I was using a full Node.js Koa server to serve what was essentially a single, dynamically generated HTML file. My own <code>Dockerfile</code> even had a commented-out <code>nginx</code> implementation that I had ignored.</li>
<li><strong>The Fix (Next Step):</strong> The plan is to refactor this into a multi-stage Docker build. A <code>node</code> stage will build the static HTML file, and a final, lightweight <code>nginx</code> stage will serve it. This will result in a smaller, faster, and more secure container.</li>
</ul>
<h4 id="chapter-9-the-kustomize-conundrum-and-service-connectivity"><strong>Chapter 9: The Kustomize Conundrum and Service Connectivity</strong><a hidden class="anchor" aria-hidden="true" href="#chapter-9-the-kustomize-conundrum-and-service-connectivity">#</a></h4>
<p>Just when I thought the deployment was stable, new challenges emerged, highlighting the intricate dance between Kustomize, ArgoCD, and inter-service communication.</p>
<p><strong>1. The Stubborn ImagePullBackOff: Kustomize Overlays vs. Reality</strong></p>
<p>Even after correctly configuring GCR project IDs and ensuring my CI/CD pushed SHA-tagged images, pods were still stuck in <code>ImagePullBackOff</code>, trying to pull images with the <code>staging-initial</code> tag.</p>
<ul>
<li><strong>The Problem:</strong> My <code>base</code> Kustomize configuration specified <code>newTag: staging-initial</code> for the images. Kubernetes tried to pull these images immediately. However, my CI/CD pipeline builds images with SHA-based tags (e.g., <code>staging-b287111</code>) and then updates the <code>kustomization.yaml</code> in the <code>overlays/staging</code> directory with these new, correct tags. The <code>staging-initial</code> image simply didn&rsquo;t exist in my GCR.</li>
<li><strong>The Docker Hub Detour (and why it was a band-aid):</strong> In an attempt to unblock, I temporarily switched the <code>overlays/staging/kustomization.yaml</code> to point to Docker Hub images. This was a quick fix to get <em>any</em> image pulled, but it wasn&rsquo;t the long-term solution as my CI/CD pipeline pushes to GCR.</li>
<li><strong>The Strategic Merge Patch Breakthrough:</strong> The real solution came from understanding Kustomize&rsquo;s patching mechanisms more deeply. Instead of relying on the <code>images:</code> field (which seemed to be problematic for direct overrides), I created explicit strategic merge patch files for each deployment (<code>broadcaster</code>, <code>todo-backend</code>, <code>todo-frontend</code>). These patches directly set the <code>image</code> field of the container to the full GCR image path <em>including the SHA tag</em> (e.g., <code>gcr.io/dwk-gke-458706/broadcaster:staging-b287111</code>). This forced the Kubernetes Deployment objects to update their image references.</li>
<li><strong>The Reversion:</strong> Once this explicit patching strategy proved successful in getting the pods to pull the correct images, I reverted these hardcoded strategic merge patches. The goal is for the CI/CD pipeline to dynamically update the <code>images:</code> field in <code>overlays/staging/kustomization.yaml</code>, and for ArgoCD to correctly apply that. This is the next phase of refinement.</li>
</ul>
<p><strong>2. The Broadcaster&rsquo;s Identity Crisis: NATS Connectivity</strong></p>
<p>Even after the image issues were resolved, the <code>broadcaster</code> pod was still crashing with <code>getaddrinfo ENOTFOUND my-nats</code>.</p>
<ul>
<li><strong>The Problem:</strong> The <code>broadcaster</code> service was trying to connect to the NATS server using the service name <code>my-nats</code>. While <code>my-nats</code> exists, it was deployed in the <code>default</code> namespace, and the <code>broadcaster</code> was in the <code>staging</code> namespace. Kubernetes service discovery requires the fully qualified domain name (FQDN) for cross-namespace communication.</li>
<li><strong>The Solution:</strong> I updated the <code>NATS_URL</code> environment variable in the <code>broadcaster-deployment.yaml</code> to use the FQDN: <code>nats://my-nats.default.svc.cluster.local:4222</code>. This allowed the <code>broadcaster</code> to correctly resolve and connect to the NATS service.</li>
<li><strong>Staging Mode Confirmation:</strong> This also confirmed that my existing <code>broadcaster-patch.yaml</code> (which sets <code>DISCORD_WEBHOOK_URL</code> to an empty string) was correctly being applied, ensuring the broadcaster runs in a safe, log-only mode in staging.</li>
</ul>
<h3 id="overall-application-look">Overall Application Look<a hidden class="anchor" aria-hidden="true" href="#overall-application-look">#</a></h3>
<p>Here are some screenshots of the overall application:</p>
<p><img alt="Overall Application Look 1" loading="lazy" src="/blog/my-todo-app-journey/bp1.png"></p>
<p><img alt="Overall Application Look 2" loading="lazy" src="/blog/my-todo-app-journey/bp2.png"></p>
<p><img alt="Overall Application Look 3" loading="lazy" src="/blog/my-todo-app-journey/bp3.png"></p>
<p><img alt="Overall Application Look 4" loading="lazy" src="/blog/my-todo-app-journey/bp4.png"></p>
<p><img alt="Overall Application Look 5" loading="lazy" src="/blog/my-todo-app-journey/bp5.png"></p>
<h4 id="conclusion-i-didn"><strong>Conclusion: I Didn&rsquo;t Just Build an App, I Built a Platform</strong><a hidden class="anchor" aria-hidden="true" href="#conclusion-i-didn">#</a></h4>
<p>This Todo app journey was my crucible. It took me from the basics of Pods and Services to the advanced, real-world challenges of stateful sets, multi-environment deployments, and secure GitOps workflows. I didn&rsquo;t just learn a list of commands; I learned how to think like a cloud-native engineer.</p>
<p>I now have the skills to design, build, and manage a resilient, observable, and fully automated system on Kubernetes. This project is more than just a portfolio piece; it&rsquo;s a testament to the deep, practical knowledge that can only be gained by building. And I&rsquo;m ready to build what&rsquo;s next.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Kyle Mapue</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
